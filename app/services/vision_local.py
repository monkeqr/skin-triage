import base64
from io import BytesIO
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch

class LocalVisionService:
    def __init__(self):
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π Vision-–º–æ–¥–µ–ª–∏ (BLIP)...")
        # BLIP Base –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è captioning
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        print(f"‚úÖ Vision-–º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –Ω–∞ {self.device}")

    def _image_from_base64(self, b64_str: str) -> Image.Image:
        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ data:image/jpeg;base64, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if "," in b64_str:
            b64_str = b64_str.split(",")[1]
        image_data = base64.b64decode(b64_str)
        return Image.open(BytesIO(image_data)).convert('RGB')

    def _generate_description(self, raw_image, text_prompt: str = None) -> str:
        """
        –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ. 
        –ï—Å–ª–∏ –µ—Å—Ç—å text_prompt, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —ç—Ç—É —Ñ—Ä–∞–∑—É, –≥–ª—è–¥—è –Ω–∞ —Ñ–æ—Ç–æ.
        """
        if text_prompt:
            # Conditional generation: "The skin texture is..." -> model completes it
            inputs = self.processor(raw_image, text_prompt, return_tensors="pt").to(self.device)
        else:
            # Unconditional generation: –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—à–∏ —á—Ç–æ –≤–∏–¥–∏—à—å
            inputs = self.processor(raw_image, return_tensors="pt").to(self.device)

        out = self.model.generate(**inputs, max_new_tokens=50, min_length=10)
        return self.processor.decode(out[0], skip_special_tokens=True)

    def analyze_image(self, b64_image: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∑–∞–¥–∞–≤–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –º–æ–¥–µ–ª–∏.
        """
        raw_image = self._image_from_base64(b64_image)
        
        # 1. –û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (Base caption)
        general_desc = self._generate_description(raw_image)
        
        # 2. –£—Ç–æ—á–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π —á–µ—Ä–µ–∑ Conditional Prompting
        # –ú—ã –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
        texture_desc = self._generate_description(raw_image, "a close up photo of skin texture which is")
        color_desc = self._generate_description(raw_image, "the color of the skin rash is")
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π "–ø—Ä–æ–º–ø—Ç" –¥–ª—è LLM
        features_report = (
            f"Visual Analysis Report:\n"
            f"1. General View: {general_desc}\n"
            f"2. Texture Details: {texture_desc}\n"
            f"3. Coloration: {color_desc}"
        )
        
        return features_report

local_vision_service = LocalVisionService()